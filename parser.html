<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">

    <title>Laika</title>

    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="css/bootstrap.css" rel="stylesheet">
    <link href="css/docs.css" rel="stylesheet">

    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

  </head>

  <body data-spy="scroll" data-target=".toc" data-offset="200">


  <div class="container">

    <!-- Docs nav
    ================================================== -->
    <div class="row">
      <div class="span4 toc" >
        
        <ul class="nav nav-list affix">
          <div class="nav-top"><img src="img/laika-top.png"/></div>
          <li class="nav-header">Introduction</li>
          <li><a href="index.html">Getting Started</a></li>
          <li><a href="index.html#available-features">Available Features</a></li>
          <li><a href="index.html#planned-features">Planned Features</a></li>
          <li><a href="index.html#design-principles">Design Principles</a></li>
          
          <li class="nav-header">Transformation Basics</li>
          <li><a href="basics.html">Transform API</a></li>
          <li><a href="basics.html#parse">Parse API</a></li>
          <li><a href="basics.html#render">Render API</a></li>
          
          <li class="nav-header">Supported Markup</li>
          <li><a href="markup.html">Markdown</a></li>
          
          <li class="nav-header">Customizing Renderers</li>
          <li><a href="customize.html">Using the Transform API</a></li>
          <li><a href="customize.html#render">Using the Render API</a></li>
          <li><a href="customize.html#writer">The Writer APIs</a></li>
          
          <li class="nav-header">Document Tree Rewriting</li>
          <li><a href="tree-rewriting.html">Using the rewrite Method</a></li>
          <li><a href="tree-rewriting.html#transform">Using the Transform API</a></li>
          
          <li class="nav-header">Implementing a Parser</li>
          <li class="active"><a href="#api">Providing an API</a></li>
          <li><a href="#markup-parsers">Trait MarkupParsers</a></li>
          <li><a href="#inline-parsers">Trait InlineParsers</a></li>
          <li><a href="#block-parsers">Trait BlockParsers</a></li>
          
          <li class="nav-header">Implementing a Renderer</li>
          <li><a href="renderer.html">Providing an API</a></li>
          <li><a href="renderer.html#function">The Render Function</a></li>
          <div class="nav-bottom"><img src="img/laika-bottom.png" border="1"/></div>
        </ul>
      </div>
      
      
      
      
      <div class="span8">

        <div>
  <div class="page-header">
    <h1>Implementing a Parser</h1>
  </div>
  <p>This document describes the best practices for adding an entirely new parser to the library.
  It is only useful if you either plan to implement a parser for a markup language not (yet)
  supported by Laika, want to replace one of the existing parsers, or are just
  curious about the inner workings of the library. None of the information here is required
  for standard usage of Laika.</p>
  <p>The contract a parser has to adhere to is quite simple, it has to be a function
  with this signature:</p>
  <pre>Input =&gt; Document
</pre>
  <p><code>Input</code> is a little IO abstraction provided by Laika so that you do not have to
  deal with the details of whether the text comes from a string or file or other 
  types of streams.</p>
  <p>Document is a case class representing the root node of the document tree.</p>
  <p>The way you implement this function depends entirely on your requirements and preferences.
  You may use the parser combinators from the Scala SDK or some other parsing technology.
  You may also use the base traits Laika provides when using combinators, or you may ignore them.</p>
  <p>Only if you intend to create a parser that you&apos;d want to contribute to the Laika core, it would
  be required to build upon the available base traits unless this is impractical for some reason.  <br></p>
  <p>If you are not using the base traits all you need to know from this document is the next section
  about providing an API. The remaining sections deal with the optional base traits provided by Laika.</p>
  <h2 id="api">Providing an API</h2>
  <p>When you build a new parser you should provide the following features for your users:</p>
  <ul>
    <li><p>An easy way to use your parser with the Transform API</p></li>
    <li><p>An easy way to use it with the Parse API</p></li>
    <li><p>A fluent API for specifying options (in case your parser is configurable)</p></li>
  </ul>
  <p>The first two come for free when you create an object that extends <code>Input =&gt; Document</code>.
  The built-in <code>Markdown</code> object is an example. Since it does extend that function,
  you can easily use it in expressions like this:</p>
  <pre>val transform = Transform from Markdown to HTML
</pre>
  <p>When you want to specify options (<code>Markdown</code> currently has only one) you can do this
  inline:</p>
  <pre>val transform = Transform from (Markdown withVerbatimHTML) to HTML</pre>
  <p>You can achieve this by providing a trait that offers all the available configuration
  hooks and returns <code>this</code> for each of these methods for easy chaining. Additionally
  you create a companion object that represents the default configuration.</p>
  <p>This is how the trait and object look for the Markdown parser as an example (Scaladoc
  comments removed for brevity):</p>
  <pre>package laika.parse.markdown

import laika.io.Input
import laika.parse.markdown.html.HTMLParsers
import laika.tree.Elements.Document

class Markdown private (verbatimHTML: Boolean) extends (Input =&gt; Document) {

  def withVerbatimHTML = new Markdown(true)

  private lazy val parser = {
    if (verbatimHTML) new BlockParsers with InlineParsers with HTMLParsers
    else              new BlockParsers with InlineParsers
  }

  def apply (input: Input) = parser.parseDocument(input.asParserInput)

}

object Markdown extends Markdown(false) </pre>
  <p>As you see, all the low-level parsing details are left in the three traits, this is
  just a wrapper for providing a convenient public API.</p>
  <p>It calls <code>asParserInput</code> on the <code>Input</code> instance which is the most convenient way
  if you use parser combinators, as it directly gives you a <code>Reader[Char]</code> no matter
  where the text is actually read from. When not using combinators, you can use
  <code>Input.asReader</code> to obtain a plain <code>java.io.Reader</code>.</p>
  <h2 id="markup-parsers">Trait MarkupParsers</h2>
  <p>This is the base trait for both the <code>InlineParsers</code> and <code>BlockParsers</code> sub-traits,
  but its parsers can also be used directly.</p>
  <p>The functionality provided by these parsers is not strictly required for implementing
  parsers for any markup language. But it comes with several advantages over using
  only the parsers from the Scala SDK. The parsers are more tailored for the special 
  requirements of parsing text markup, which is quite different from parsing programming 
  languages for example. </p>
  <p>In particular for parsing inline markup (like *this* for adding emphasis) Laika&apos;s parsers deviate
  from the standard approach of combinators, which in this case would often mean to build
  a long list of (flat) choices of (often) regex parsers which are all tried on each character.
  The downside of this approach is that this is often quite slow, and not easily extensible, 
  if you want to support new or customized markup for an existing parser without touching the parser
  implementation.</p>
  <p>For typical basic regex parsers there is usually a corresponding option in the <code>MarkupParsers</code>
  trait. You can see the full list of provided parsers in the <a href="api/#laika.parse.MarkupParsers">Scaladoc</a>.
  We&apos;ll just show a few examples here:</p>
  <p>Parsing three or more lower-case characters:</p>
  <pre>&quot;[a-z]{3,}&quot;.r              // regex

anyIn(&apos;a&apos; to &apos;z&apos;) min 3    // Laika alternative
</pre>
  <p>Parsing any character apart from space or tab:</p>
  <pre>&quot;[^ \t]*&quot;.r               // regex

anyBut(&apos; &apos;,&apos;\t&apos;)          // Laika alternative
</pre>
  <p>Note that for the Laika base parsers the default is to parse any number of characters,
  as this is the most common case. To read just one you can write:</p>
  <pre>anyBut(&apos; &apos;,&apos;\t&apos;) take 1
</pre>
  <p>The parsers of this trait will be faster than regex parsers in many scenarios, but
  there will be edge cases where it is the other way round. To be really sure it&apos;s best
  to do some benchmarks first. </p>
  <h2 id="inline-parsers">Trait InlineParsers</h2>
  <p>Laika parses text markup in two phases: in the first phase it only looks for markup which is
  significant for identifying a block type (like a blockquote, an ordered list, a code block
  or a regular paragraph for example). It then further parses the text of each block to look
  for inline markup, which is what this trait does.</p>
  <p>Like described in the previous section, Laika inline parsers avoid the performance bottleneck
  of trying a long list of choices for each character. Instead it builds a map of available
  span parsers, mapping from the first character to the parser, and then performs a simple
  map lookup for each character. If no parser is mapped to the character it simply continues
  reading. This works for nested structures, too.</p>
  <p>The main method in the <code>InlineParsers</code> trait that block parsers use to parse their text is:</p>
  <pre>def parseInline (source: String, spanParsers: Map[Char, Parser[Span]])
</pre>
  <p>The second parameter is the parser map we mentioned. The return type is <code>List[Span]</code>, 
  with <code>Span</code> being a sub-trait of <code>Element</code>, the base class for all node types. 
  Examples for case classes mixing in <code>Span</code> are <code>Link</code>, <code>Image</code> or <code>Emphasized</code>.</p>
  <p>For Markdown this map is created like this:</p>
  <pre>protected def newSpanParserMap = Map(
  &apos;*&apos; -&gt; (strong(&apos;*&apos;) | em(&apos;*&apos;)),    
  &apos;_&apos; -&gt; (strong(&apos;_&apos;) | em(&apos;_&apos;)),
  &apos;`&apos; -&gt; (codeEnclosedByDoubleChar | codeEnclosedBySingleChar), 
  &apos;\\&apos;-&gt; (escapedChar ^^ { Text(_) }),
  &apos;[&apos; -&gt; link,
  &apos;&lt;&apos; -&gt; simpleLink,
  &apos;!&apos; -&gt; image
)
</pre>
  <p>The parsers mapped to the characters are then pretty standard, apart from the fact
  that they are usually built upon the base parsers provided by the <code>MarkupParsers</code>
  trait. These parsers must not look for
  the initial character as this will be consumed by the map lookup already. And they
  are still allowed to fail, in which case the special character will be treated as
  normal text input.</p>
  <p>Sub-traits can override this method and add more parsers to the map.</p>
  <p>If any of the parsers need to parse nested structures, <code>InlineParsers</code> comes
  with an additional method that allows to specify both, a condition (in form of a parser)
  for when the span ends and a map for available nested parsers, which may or may not
  be identical to the map for top level spans, depending on the markup specification.
  The signature is as follows:</p>
  <pre>def spans (parser: TextParser, nested: Map[Char, Parser[Span]]) </pre>
  <p><code>TextParser</code> is a sub-trait of <code>Parser[String]</code>, and the type of parser produced
  by most methods in <code>MarkupParsers</code>. The rules for the map are identical
  to those for top-level spans. The text parser parses the current span, but may get
  suspended for parsing nesting structures based on the parser map. </p>
  <h2 id="block-parsers">Trait BlockParsers</h2>
  <p>This is the base trait for parsing blocks, the first phase of the two-phase parsing
  process. It adds a few utility parsers like <code>eol</code> (end-of-line), <code>blankLine</code>
  or <code>restOfLine</code>. See the [Scaladoc][inline-scaladoc] for details on those.</p>
  <p>If you mix in this trait you have to implement the two abstract parsers:</p>
  <p>  def topLevelBlock: Parser[Block]</p>
  <p>  def nestedBlock: Parser[Block]</p>
  <p>This can be a parser build as a list of choices. In contrast to the inline
  parsers the performance impact is less of a concern here. The names should be
  self-explanatory. Depending on the concrete markup language specification
  these two parsers may be identical, if each block type is allowed as a top-level
  or nested block.</p>
  <p> Like <code>Span</code>, <code>Block</code> is a sub-trait of <code>Element</code>, the base class for all node types. 
  Examples for case classes mixing in <code>Block</code> are <code>Paragraph</code>, <code>CodeBlock</code> or <code>OrderedList</code>.</p>
  <p>Finally this trait provides an (entirely optional) convenience method for creating
  a typical block parser. This is the signature:</p>
  <pre>def block (firstLinePrefix: Parser[Any], 
           linePrefix: Parser[Any], 
           nextBlockPrefix: Parser[Any]): Parser[List[String]]</pre>
  <p>It allows to parse a block based on three simple conditions, provided in form of
  the three parser parameters: detecting the first line of a block, any subsequent
  line and finally whether the block continues after a blank line has been seen.
  Often a blank line marks the end of a block, but there are exceptions, like code
  blocks or list items that span multiple parapraphs.</p>
</div>        

      </div>
    </div>

  </div>


    <!-- javascript
    ================================================== -->
    <script src="js/jquery-1.8.3.min.js"></script>
    <script src="js/bootstrap.min.js"></script>


</body></html>