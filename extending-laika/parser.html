<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">

    <title>Laika</title>

    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Customizable and extensible toolkit for transforming lightweight markup languages to various types of output formats, written in Scala" />
    <meta name="keywords" content="scala, text, markup, markdown, restructuredtext, parser, transformer, html, template engine, site generation, open-source" />

    <link href="../css/bootstrap.css" rel="stylesheet">
    <link href="../css/docs.css" rel="stylesheet">

    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

  </head>

  <body data-spy="scroll" data-target=".toc" data-offset="200">


  <div class="container">

    <!-- Docs nav
    ================================================== -->
    <div class="row">
      <div class="span4 toc" >
        
        <ul class="nav nav-list affix">
          <div class="nav-top"><img src="../img/laika-top.png"/></div>
          
          
          <li class="nav-header">Introduction</li>
          <li><a href="../introduction/intro.html">Overview</a></li>
          <li><a href="../introduction/architecture.html">Architecture</a></li>
          <li class="nav-header">Using Laika</li>
          <li><a href="../using-laika/sbt.html">Using the sbt Plugin</a></li>
          <li><a href="../using-laika/embedded.html">Using Laika Embedded</a></li>
          <li><a href="../using-laika/markup.html">Supported Markup</a></li>
          <li><a href="../using-laika/output.html">Supported Output Formats</a></li>
          <li><a href="../using-laika/structure.html">Document Structure</a></li>
          <li><a href="../using-laika/templates.html">Templates</a></li>
          <li class="nav-header">Customizing Laika</li>
          <li><a href="../customizing-laika/customize-rendering.html">Customizing Renderers</a></li>
          <li><a href="../customizing-laika/tree-rewriting.html">Document Tree Rewriting</a></li>
          <li><a href="../customizing-laika/parsing-rendering.html">Separate Parsing and Rendering</a></li>
          <li class="nav-header">Extending Laika</li>
          <li><a href="directive.html">Directives</a></li>
          <li class="active"><a href="#">Parsers</a></li>
          <li><a href="renderer.html">Renderers</a></li>
          <li><a href="extending-rst.html">Extending reStructuredText</a></li>
          
          <li class="nav-header">Project Links</li>
          <li><a href="http://github.com/planet42/Laika">Source Code</a></li>
          <li><a href="../api/">API Documentation</a></li>
          <li><a href="http://github.com/planet42/Laika/issues">Issue Tracker</a></li>
          <li><a href="http://planet42.org/">Transformer Web Tool</a></li>
          <li class="follow"><a href="https://twitter.com/_planet42" class="twitter-follow-button" data-show-count="false" data-show-screen-name="false" data-dnt="true">Follow @_planet42</a>
          <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script></li>
          
          <div class="nav-bottom"><img src="../img/laika-bottom.png" border="1"/></div>
        </ul>
        
      </div>
      
      <div class="span8" id="top">

        <div class="page-header">
          <h1>Implementing a Parser</h1>
        </div>
        <p>This document describes the best practices for adding an entirely new parser to the toolkit.
        It is only useful if you either plan to implement a parser for a markup language not (yet)
        supported by Laika, want to replace one of the existing parsers, or are just
        curious about the inner workings of the library. None of the information here is required
        for standard usage of Laika.</p>
        
        <h2 id="factory-contract" class="section">Factory Contract</h2>
        <p>The contract a parser factory has to adhere to is quite simple, it has mix in the following
        trait:</p>
        <pre>trait ParserFactory {

  def fileSuffixes: Set[String]

  def newParser: Input =&gt; Document
  
}
</pre>
        <p>The <code>fileSuffixes</code> method should simply return the set of supported file
        suffixes (without the &#39;.&#39;). For Markdown this would be <code>Set(&quot;md&quot;,&quot;markdown&quot;)</code>
        for example. It is not recommended to support generic suffixes like <code>txt</code> as this
        could lead to conflicts with other parsers.</p>
        <p>The <code>newParser</code> method is the actual factory method. It should return
        a function that is safe to use repeatedly and concurrently.</p>
        <p><code>Input</code> is a little IO abstraction provided by Laika so that you do not have to
        deal with the details of whether the text comes from a string or file or other 
        types of streams.</p>
        <p><code>Document</code> is a case class representing the root node of the document tree.</p>
        <p>The way you implement this function depends entirely on your requirements and preferences.
        You may use the parser combinators from Laika or some other parsing technology.</p>
        <p>Only if you intend to create a parser that you&#39;d want to contribute to the Laika core, it would
        be required to build upon the available base traits unless this is impractical for some reason.  <br></p>
        <p>If you are not using the base traits all you need to know from this document is the next section
        about providing an API. The remaining sections deal with the optional base traits provided by Laika.</p>
        
        <h2 id="providing-an-api" class="section">Providing an API</h2>
        <p>When you build a new parser you should provide the following features for your users:</p>
        <ul>
          <li>
            <p>An easy way to use your parser with the Transform API</p>
          </li>
          <li>
            <p>An easy way to use it with the Parse API</p>
          </li>
          <li>
            <p>A fluent API for specifying options (in case your parser is configurable)</p>
          </li>
        </ul>
        <p>The first two come for free when you create an object that extends <code>Input =&gt; RawDocument</code>.
        The built-in <code>Markdown</code> object is an example. Since it does extend that function,
        you can easily use it in expressions like this:</p>
        <pre>val transform = Transform from Markdown to HTML
</pre>
        <p>When you want to specify options (<code>Markdown</code> currently has only one) you can do this
        inline:</p>
        <pre>val transform = Transform from (Markdown withVerbatimHTML) to HTML</pre>
        <p>You can achieve this by providing a trait that offers all the available configuration
        hooks and returns <code>this</code> for each of these methods for easy chaining. Additionally
        you create a companion object that represents the default configuration.</p>
        <p>As an example, this is how a simplified class and object for the Markdown parser 
        could look (Scaladoc comments, imports and various extension hooks removed for brevity):</p>
        <pre>package laika.parse.markdown

class Markdown private (verbatimHTML: Boolean, isStrict: Boolean) 
                                               extends ParserFactory {

  val fileSuffixes = Set(&quot;md&quot;, &quot;markdown&quot;)
  
  def withVerbatimHTML = new Markdown(true, isStrict)
  
  def strict = new Markdown(verbatimHTML, true)

  private lazy val parser = {
    lazy val blockDirectives = ...
    lazy val spanDirectives = ...
    
    new RootParser(blockDirectives, spanDirectives, verbatimHTML, isStrict)
  }

  val newParser: Input =&gt; Document = 
     (input: Input) =&gt; parser.parseDocument(input.asParserInput, input.path)

}

object Markdown extends Markdown(false, false) </pre>
        <p>As you see, all the low-level parsing details are left in the root parser, this is
        just a wrapper for providing a convenient public API. Support for reStructuredText
        is implemented in a similar way.</p>
        <p>It calls <code>asParserInput</code> on the <code>Input</code> instance which is the most convenient way
        if you use parser combinators, as it directly gives you a <code>ParserContext</code> no matter
        where the text is actually read from. When not using combinators, you can use
        <code>Input.asReader</code> to obtain a plain <code>java.io.Reader</code>.</p>
        
        <h2 id="text-parsers" class="section">Text Parsers</h2>
        <p>The functionality provided by these parsers is not strictly required for implementing
        parsers for any markup language. But the parsers are convenient helpers as they are more tailored 
        for the special requirements of parsing text markup, which is quite different from parsing programming 
        languages for example. </p>
        <p>In particular for parsing inline markup (like *this* for adding emphasis) Laika&#39;s parsers deviate
        from the standard approach of combinators, which in this case would often mean to build
        a long list of (flat) choices of (often) regex parsers which are all tried on each character.
        The downside of this approach is that this is often quite slow, and not easily extensible, 
        if you want to support new or customized markup for an existing parser without touching the parser
        implementation.</p>
        <p>For typical basic regex parsers there is usually a corresponding option in the <code>TextParsers</code>
        object. You can see the full list of provided parsers in the <a href="../api/#laika.parse.core.text.TextParsers">Scaladoc</a>.
        We&#39;ll just show a few examples here:</p>
        <p>Parsing three or more lower-case characters:</p>
        <pre>&quot;[a-z]{3,}&quot;.r              // regex

anyIn(&#39;a&#39; to &#39;z&#39;) min 3    // Laika alternative
</pre>
        <p>Parsing any character apart from space or tab:</p>
        <pre>&quot;[^ \t]*&quot;.r               // regex

anyBut(&#39; &#39;,&#39;\t&#39;)          // Laika alternative
</pre>
        <p>Note that for the Laika base parsers the default is to parse any number of characters,
        as this is the most common case. To read just one you can write:</p>
        <pre>anyBut(&#39; &#39;,&#39;\t&#39;) take 1
</pre>
        <p>The parsers of this trait will be faster than regex parsers in many scenarios, but
        there will be edge cases where it is the other way round. To be really sure it&#39;s best
        to do some benchmarks first. </p>
        
        <h2 id="span-parsers" class="section">Span Parsers</h2>
        <p>Laika parses text markup in two phases: in the first phase it only looks for markup which is
        significant for identifying a block type (like a blockquote, an ordered list, a code block
        or a regular paragraph for example). It then further parses the text of each block to look
        for inline markup, which is what this trait does.</p>
        <p>Like described in the previous section, Laika inline parsers avoid the performance bottleneck
        of trying a long list of choices for each character. Instead it builds a map of available
        span parsers, mapping from the first character to the parser, and then performs a simple
        map lookup for each character. If no parser is mapped to the character it simply continues
        reading. This works for nested structures, too.</p>
        <p>To provide all span parsers for a custom markup parser based on Laika&#39;s <code>RootParserBase</code>
        you only have to implement the abstract method</p>
        <pre>protected lazy val spanParsers: Map[Char,Parser[Span]]
</pre>
        <p>For Markdown the default span parser map is created like this:</p>
        <pre>protected lazy val spanParsers: Map[Char, Parser[Span]] = Map(
    &#39;*&#39; -&gt; (strong(&#39;*&#39;) | em(&#39;*&#39;)),    
    &#39;_&#39; -&gt; (strong(&#39;_&#39;) | em(&#39;_&#39;)),
    &#39;`&#39; -&gt; (literalEnclosedByDoubleChar | literalEnclosedBySingleChar), 
    &#39;\\&#39;-&gt; (lineBreak | (escapedChar ^^ { Text(_) })),
    &#39;[&#39; -&gt; link,
    &#39;&lt;&#39; -&gt; simpleLink,
    &#39;!&#39; -&gt; image
  )
</pre>
        <p>The parsers mapped to the characters are then pretty standard, apart from the fact
        that they are usually built upon the base parsers provided by the <code>TextParsers</code>
        object and the <code>RecursiveSpanParsers</code> trait. These parsers must not look for
        the initial character as this will be consumed by the map lookup already. And they
        are still allowed to fail, in which case the special character will be treated as
        normal text input.</p>
        
        <h2 id="block-parsers" class="section">Block Parsers</h2>
        <p>Laika offers a <code>BlockParsers</code> object with convenience methods for creating
        a typical block parser. This is the signature of the first one:</p>
        <pre>def block (firstLinePrefix: Parser[Any], 
           linePrefix: Parser[Any], 
           nextBlockPrefix: Parser[Any]): Parser[List[String]]</pre>
        <p>It allows to parse a block based on three simple conditions, provided in form of
        the three parser parameters: detecting the first line of a block, any subsequent
        line and finally whether the block continues after a blank line has been seen.
        Often a blank line marks the end of a block, but there are exceptions, like code
        blocks or list items that span multiple parapraphs.</p>
        <p>Finally there is a second utility that can be used for indented blocks:</p>
        <pre>def indentedBlock (minIndent: Int = 1,
          linePredicate: =&gt; Parser[Any] = success(()),
          endsOnBlankLine: Boolean = false,
          firstLineIndented: Boolean = false,
          maxIndent: Int = Int.MaxValue): Parser[String]</pre>
        <p>Like the other utility it allows to specify a few predicates. This method
        is not used for parsing Markdown&#39;s indented blocks, though, as Markdown has
        a very special way of treating whitespace.</p>
        <p>To provide all block parsers for a custom markup parser based on Laika&#39;s <code>RootParserBase</code>
        you have to implement the abstract methods</p>
        <pre>protected def topLevelBlock: Parser[Block]

protected def nestedBlock: Parser[Block]

protected def nonRecursiveBlock: Parser[Block]</pre>
        <p>The first two methods allow to make a distinction between a block that is only
        allowed to occur on the top level and those which can appear anywhere. 
        Of course any block can potentially appear in both parsers.</p>
        <p>The third method <code>nonRecursiveBlock</code> is rarely ever used, it exists
        as a safeguard against malicious input. If a certain (configurable) nest level
        has been reached, only block elements that do not potentially contain further
        nested blocks are considered for subsequent parsing.</p>      

      </div>
    </div>

  </div>

</body></html>
